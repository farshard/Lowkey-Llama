# Local LLM Chat Interface

A Streamlit-based chat interface with API support for interacting with local LLMs through Ollama. This project provides both a web-based chat interface and a REST API for integrating local language models into your applications.

## Architecture

```mermaid
graph TD
    A[Web UI - Streamlit] -->|HTTP| B[FastAPI Server]
    B -->|API Calls| C[Ollama Service]
    C -->|Model Inference| D[Local LLM Models]
    E[External Apps] -->|REST API| B
```

## Features

- ğŸŒ Web-based chat interface using Streamlit
- ğŸš€ FastAPI-based REST API
- ğŸ”„ Integration with Ollama for local LLM inference
- ğŸ³ Docker support for easy deployment
- âš¡ Real-time text generation
- ğŸ“š Support for multiple LLM models

## Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai/) installed and running
- Docker (optional for container deployment)

## Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chat_app.py      # Streamlit chat interface
â”‚   â”œâ”€â”€ api_server.py    # FastAPI REST API
â”‚   â””â”€â”€ setup.py         # Package setup
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ docs/                # Documentation
â”œâ”€â”€ models/              # Model configurations
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ Dockerfile          # Container definition
â””â”€â”€ docker-compose.yml  # Container orchestration
```

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/voolyvex/Local-LLM.git
   cd Local-LLM
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Make sure Ollama is running with your desired model:
   ```bash
   ollama run mistral
   ```

## Usage

### Running the Chat Interface

```bash
streamlit run src/chat_app.py
```

### Running the API Server

```bash
python src/api_server.py
```

### Using Docker

```bash
docker-compose up
```

## API Endpoints

- `POST /api/generate`: Generate text from a prompt
- `GET /api/models`: List available models

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is open source and available under the MIT License.
